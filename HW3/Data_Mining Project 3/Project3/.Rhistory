##LDA
lda.fit <- lda( V6 ~ . , data=mydata_train )
lda.pred.train <- predict(lda.fit , newdata=mydata_train)
lda.pred.test <- predict(lda.fit , newdata=mydata_test)
y_true_train <- (mydata_train$v6)
y_true_test <- (mydata_test$v6)
y_hat_train <- lda.pred.train$class
y_hat_test <- lda.pred.test$class
LDA_train_error = sum(abs(y_true_train -y_hat_train))/length(y_true_train)
LDA_test_error = sum(abs(y_true_test -y_hat_test))/length(y_true_test)
mydata_train
lda.fit <- lda( V6 ~ . , data=mydata_train )
lda.pred.train <- predict(lda.fit , newdata=mydata_train)
lda.pred.test <- predict(lda.fit , newdata=mydata_test)
lda.pred.tra
lda.pred.train
mydata_train$v6
y_true_train <- as.numeric(mydata_train$V6)
y_true_test <- as.numeric(mydata_test$V6)
data_input = read.csv("DiabetesAndrews36_1.csv", header=FALSE)
mydata <- data_input
pairs(mydata[1:5], main = "Diabetes", pch = 21, bg = c("red", "green3", "blue")[unclass(mydata$V6)], lower.panel=NULL, labels=c("Gl","Ins","SS","Wgt","Pl"), font.labels=2, cex.labels=4.5)
##Correlation
cor(mydata[,1:5])
##Train and test split
train = sample(1:nrow(mydata),nrow(mydata)*.80)
mydata_train = mydata[train,]
mydata_test = mydata[-train,]
##LDA
lda.fit <- lda( V6 ~ . , data=mydata_train )
lda.pred.train <- predict(lda.fit , newdata=mydata_train)
lda.pred.test <- predict(lda.fit , newdata=mydata_test)
y_true_train <- as.numeric(mydata_train$V6)
y_true_test <- as.numeric(mydata_test$V6)
y_hat_train <- lda.pred.train$class
y_hat_test <- lda.pred.test$class
LDA_train_error = sum(abs(y_true_train -y_hat_train))/length(y_true_train)
LDA_test_error = sum(abs(y_true_test -y_hat_test))/length(y_true_test)
setwd("C:/Users/Ashwin/Desktop/Data_Mining/Project3")
library(klaR)
library(ISLR)
library(MASS)
library(corrplot)
data_input = read.csv("DiabetesAndrews36_1.csv", header=FALSE)
mydata <- data_input
pairs(mydata[1:5], main = "Diabetes", pch = 21, bg = c("red", "green3", "blue")[unclass(mydata$V6)], lower.panel=NULL, labels=c("Gl","Ins","SS","Wgt","Pl"), font.labels=2, cex.labels=4.5)
##Correlation
cor(mydata[,1:5])
##Train and test split
train = sample(1:nrow(mydata),nrow(mydata)*.80)
mydata_train = mydata[train,]
mydata_test = mydata[-train,]
##LDA
lda.fit <- lda( V6 ~ . , data=mydata_train )
lda.pred.train <- predict(lda.fit , newdata=mydata_train)
lda.pred.test <- predict(lda.fit , newdata=mydata_test)
y_true_train <- as.numeric(mydata_train$V6)
y_true_test <- as.numeric(mydata_test$V6)
y_hat_train <- as.numeric(lda.pred.train$class)
y_hat_test <- as.numeric(lda.pred.test$class)
LDA_train_error = sum(abs(y_true_train -y_hat_train))/length(y_true_train)
LDA_test_error = sum(abs(y_true_test -y_hat_test))/length(y_true_test)
LDA_train_error
LDA_test_error
qda.fit <- qda(V6 ~ . , data=mydata_train )
qda.pred.train <- predict(qda.fit , newdata=mydata_train)
qda.pred.test <- predict(qda.fit , newdata=mydata_test)
y_true_train <- as.numeric(mydata_train$V6)
y_true_test <- as.numeric(mydata_test$V6)
y_hat_train <- qda.pred.train$class
y_hat_test <- qda.pred.test$class
QDA_train_error = sum(abs(y_true_train -y_hat_train))/length(y_true_train)
QDA_test_error = sum(abs(y_true_test -y_hat_test))/length(y_true_test)
QDA_train_error
qda.fit <- qda(V6 ~ . , data=mydata_train )
qda.pred.train <- predict(qda.fit , newdata=mydata_train)
qda.pred.test <- predict(qda.fit , newdata=mydata_test)
y_true_train <- as.numeric(mydata_train$V6)
y_true_test <- as.numeric(mydata_test$V6)
y_hat_train <- as.numeric(qda.pred.train$class)
y_hat_test <- as.numeric(qda.pred.test$class)
QDA_train_error = sum(abs(y_true_train -y_hat_train))/length(y_true_train)
QDA_test_error = sum(abs(y_true_test -y_hat_test))/length(y_true_test)
QDA_test_error
QDA_train_error
testdata <- r(0.98 ,122 , 544 , 186 , 184)
testdata <- c(0.98 ,122 , 544 , 186 , 184)
lda.pred.test <- predict(lda.fit , newdata=testdata)
lda.pred.test <- predict(lda.fit , newdata=t(testdata)
DSA
lda.pred.test <- predict(lda.fit , newdata=t(testdata))
testdata <- c(0.98 ,122 , 544 , 186 , 184)
df = data.frame(testdata)
lda.pred.test <- predict(lda.fit , newdata=df
)
testdata1 <- c(0.98 ,122 , 544 , 186 , 184)
testdata2 <- c(V1 , V2 ,V3 ,V4 ,V5 ,V6)
testdata2 <- c("V1" , "V2" ,"V3" ,"V4", "V5" ,"V6")
df = data.frame(testdata2 , testdata1)
testdata1 <- c(0.98 ,122 , 544 , 186 , 184,0)
df = data.frame(testdata2 , testdata1)
lda.pred.test <- predict(lda.fit , newdata=df)
testdata1 <- c(0.98 ,122 , 544 , 186 , 184,0)
testdata2 <- c("V1" , "V2" ,"V3" ,"V4", "V5" ,"V6")
lda.pred.test <- predict(lda.fit , newdata=df)
df
V1 <- c(0.98)
V2 <- c(122)
V3 <- c(544)
V4 <- c(186)
V5 <- c(184)
V6 <- c(0)
df <- data.frame(V1,V2,V3,V4,V5,V6)
qda.fit <- qda(V6 ~ . , data=mydata_train )
qda.pred.train <- predict(qda.fit , newdata=df)
qda.pred.train$class
lda.fit <- lda(V6 ~ . , data=mydata_train )
lda.pred.train <- predict(lda.fit , newdata=df)
lda.pred.train$class
data_input = read.csv("DiabetesAndrews36_1.csv", header=FALSE)
mydata <- data_input
pairs(mydata[1:5], main = "Diabetes", pch = 21, bg = c("red", "green3", "blue")[unclass(mydata$V6)], lower.panel=NULL, labels=c("Gl","Ins","SS","Wgt","Pl"), font.labels=2, cex.labels=4.5)
##Correlation
cor(mydata[,1:5])
##Train and test split
train = sample(1:nrow(mydata),nrow(mydata)*.80)
mydata_train = mydata[train,]
mydata_test = mydata[-train,]
##LDA
lda.fit <- lda( V6 ~ . , data=mydata_train )
lda.pred.train <- predict(lda.fit , newdata=mydata_train)
lda.pred.test <- predict(lda.fit , newdata=mydata_test)
y_true_train <- as.numeric(mydata_train$V6)
y_true_test <- as.numeric(mydata_test$V6)
y_hat_train <- as.numeric(lda.pred.train$class)
y_hat_test <- as.numeric(lda.pred.test$class)
LDA_train_error = sum(abs(y_true_train -y_hat_train))/length(y_true_train)
LDA_test_error = sum(abs(y_true_test -y_hat_test))/length(y_true_test)
##QDA
qda.fit <- qda(V6 ~ . , data=mydata_train )
qda.pred.train <- predict(qda.fit , newdata=mydata_train)
qda.pred.test <- predict(qda.fit , newdata=mydata_test)
y_true_train <- as.numeric(mydata_train$V6)
y_true_test <- as.numeric(mydata_test$V6)
y_hat_train <- as.numeric(qda.pred.train$class)
y_hat_test <- as.numeric(qda.pred.test$class)
QDA_train_error = sum(abs(y_true_train -y_hat_train))/length(y_true_train)
QDA_test_error = sum(abs(y_true_test -y_hat_test))/length(y_true_test)
##Prediction using LDA
V1 <- c(0.98)
V2 <- c(122)
V3 <- c(544)
V4 <- c(186)
V5 <- c(184)
V6 <- c(0)
df <- data.frame(V1,V2,V3,V4,V5,V6)
lda.fit <- lda(V6 ~ . , data=mydata_train )
lda.pred.train <- predict(lda.fit , newdata=df)
lda.pred.train$class
##Prediction using QDA
qda.fit <- qda(V6 ~ . , data=mydata_train )
qda.pred.train <- predict(qda.fit , newdata=df)
qda.pred.train$class
##LDA
lda.fit <- lda( V6 ~ . , data=mydata_train )
lda.pred.train <- predict(lda.fit , newdata=mydata_train)
lda.pred.test <- predict(lda.fit , newdata=mydata_test)
y_true_train <- as.numeric(mydata_train$V6)
y_true_test <- as.numeric(mydata_test$V6)
y_hat_train <- as.numeric(lda.pred.train$class)
y_hat_test <- as.numeric(lda.pred.test$class)
LDA_train_error = sum(abs(y_true_train -y_hat_train))/length(y_true_train)
LDA_test_error = sum(abs(y_true_test -y_hat_test))/length(y_true_test)
##QDA
qda.fit <- qda(V6 ~ . , data=mydata_train )
qda.pred.train <- predict(qda.fit , newdata=mydata_train)
qda.pred.test <- predict(qda.fit , newdata=mydata_test)
y_true_train <- as.numeric(mydata_train$V6)
y_true_test <- as.numeric(mydata_test$V6)
y_hat_train <- as.numeric(qda.pred.train$class)
y_hat_test <- as.numeric(qda.pred.test$class)
QDA_train_error = sum(abs(y_true_train -y_hat_train))/length(y_true_train)
QDA_test_error = sum(abs(y_true_test -y_hat_test))/length(y_true_test)
LDA_train_error
LDA_test_error
QDA_train_error
QDA_test_error
cor(mydata[,1:5])
library(MASS)
data(Boston)
set.seed(12345)
summary(Boston)
# Create new variable crim_med which is 1 if crime rate is above median
# and 0 if crime rate is below median
crim_med <- rep(0, length(Boston$crim))
crim_med[Boston$crim > median(Boston$crim)] <- 1
Boston <- data.frame(Boston, crim_med)
Boston$crim_med
# Create a set of box plots to check how each of the variable relates to crim_med
# ignore the chas variable and the crim variables
par(mfrow=c(2,6))
for(i in names(Boston)){
# excluding the own crime variables and the chas variable
if( grepl(i, pattern="^crim|^chas")){ next}
boxplot(eval(parse(text=i)) ~ crim_med, ylab=i, col=c("red", "blue"), varwidth=T)
}
# Create train and test set
# Use only the relevant variables
set.seed(12345)
variables = c("zn", "indus", "nox", "rm", "age", "dis", "rad", "tax", "ptratio", "black", "lstat", "medv", "crim_med")
rows = sample(x=nrow(Boston), size=.75*nrow(Boston))
train = Boston[rows, variables]
test = Boston[-rows, variables]
# Logistic Regression
lr.fit <- glm(as.factor(crim_med) ~ ., data=train, family="binomial")
lr.probs <- predict(lr.fit, test, type="response")
lr.pred <- ifelse(lr.probs>.5, "1","0")
lr.test.error <- mean(lr.pred!=test$crim_med)
lr.test.error
table(lr.pred, test$crim_med)
# LDA
lda.fit <- lda(train$crim_med ~ ., data = train)
lda.pred <- predict(lda.fit, test)
table(lda.pred$class, test$crim_med)
lda.test.error <- mean(lda.pred$class != test$crim_med)
lda.test.error
# kNN
set.seed(12345)
train.knn = train[, c("zn", "indus", "nox", "rm", "age", "dis", "rad", "tax", "ptratio", "black", "lstat", "medv")]
test.knn = test[, c("zn", "indus", "nox", "rm", "age", "dis", "rad", "tax", "ptratio", "black", "lstat", "medv")]
# kNN takes the training response variable seperately
train.knn.response = train$crim_med
# we also need the have the response variable for the test
# seperately for assesing the model later on
test.knn.response = test$crim_med
library(class)
set.seed(12345)
knn.pred = knn(train.knn, test.knn, train.knn.response, k = 1)
table(knn.pred, test.knn.response)
mean(knn.pred != test$crim_med)
knn.pred = NULL
knn.error = NULL
for(i in 1:dim(test.knn)[1]) {
set.seed(12345)
knn.pred = knn(train.knn, test.knn, train.knn.response, k = i)
knn.error[i] = mean(test$crim_med != knn.pred)
}
### find the minimum error rate
min.knn.error = min(knn.error)
print(min.knn.error)
### get the index of that error rate, which is the k
k = which(knn.error == min.knn.error)
print(k)
plot(val.errors, xlab = "k value", ylab = "error rate", pch = 19, type = "b")
plot(knn.error, xlab = "k value", ylab = "error rate", pch = 19, type = "b")
rm(list = ls())
plot(knn.error, xlab = "k value", ylab = "error rate", pch = 19, type = "b")
###########################################################################
# Using the Boston data set (ISLR package), fit classification
# models in order  to  predict  whether  a  given  suburb
# has a  crime  rate  above  or  below  the median.
# Explore logistic regression, LDA and kNN models using
# various subsets of the predictors.  Describe your findings.
##############################################################
library(MASS)
data(Boston)
set.seed(12345)
summary(Boston)
# Create new variable crim_med which is 1 if crime rate is above median
# and 0 if crime rate is below median
crim_med <- rep(0, length(Boston$crim))
crim_med[Boston$crim > median(Boston$crim)] <- 1
Boston <- data.frame(Boston, crim_med)
Boston$crim_med
# Create a set of box plots to check how each of the variable relates to crim_med
# ignore the chas variable and the crim variables
par(mfrow=c(2,6))
for(i in names(Boston)){
# excluding the own crime variables and the chas variable
if( grepl(i, pattern="^crim|^chas")){ next}
boxplot(eval(parse(text=i)) ~ crim_med, ylab=i, col=c("red", "blue"), varwidth=T)
}
# Create train and test set
# Use only the relevant variables
set.seed(12345)
variables = c("zn", "indus", "nox", "rm", "age", "dis", "rad", "tax", "ptratio", "black", "lstat", "medv", "crim_med")
rows = sample(x=nrow(Boston), size=.75*nrow(Boston))
train = Boston[rows, variables]
test = Boston[-rows, variables]
# Logistic Regression
lr.fit <- glm(as.factor(crim_med) ~ ., data=train, family="binomial")
lr.probs <- predict(lr.fit, test, type="response")
lr.pred <- ifelse(lr.probs>.5, "1","0")
lr.test.error <- mean(lr.pred!=test$crim_med)
lr.test.error
table(lr.pred, test$crim_med)
# LDA
lda.fit <- lda(train$crim_med ~ ., data = train)
lda.pred <- predict(lda.fit, test)
table(lda.pred$class, test$crim_med)
lda.test.error <- mean(lda.pred$class != test$crim_med)
lda.test.error
# kNN
set.seed(12345)
train.knn = train[, c("zn", "indus", "nox", "rm", "age", "dis", "rad", "tax", "ptratio", "black", "lstat", "medv")]
test.knn = test[, c("zn", "indus", "nox", "rm", "age", "dis", "rad", "tax", "ptratio", "black", "lstat", "medv")]
# kNN takes the training response variable seperately
train.knn.response = train$crim_med
# we also need the have the response variable for the test
# seperately for assesing the model later on
test.knn.response = test$crim_med
library(class)
set.seed(12345)
knn.pred = knn(train.knn, test.knn, train.knn.response, k = 1)
table(knn.pred, test.knn.response)
mean(knn.pred != test$crim_med)
knn.pred = NULL
knn.error = NULL
for(i in 1:dim(test.knn)[1]) {
set.seed(12345)
knn.pred = knn(train.knn, test.knn, train.knn.response, k = i)
knn.error[i] = mean(test$crim_med != knn.pred)
}
### find the minimum error rate
min.knn.error = min(knn.error)
print(min.knn.error)
### get the index of that error rate, which is the k
k = which(knn.error == min.knn.error)
print(k)
plot(knn.error, xlab = "k value", ylab = "error rate", pch = 19, type = "b")
plot(knn.error, xlab = "k value", ylab = "error rate", pch = 19, type = "b")
plot(knn.error, xlab = "k value", ylab = "error rate", pch = 19, type = "b")
plot(knn.error, xlab = "k value", ylab = "error rate", pch = 19, type = "b")
plot(knn.error, xlab = "k value", ylab = "error rate", pch = 19, type = "b")
plot(knn.error, xlab = "k value", ylab = "error rate", pch = 19, type = "b")
plot(knn.error, xlab = "k value", ylab = "error rate", pch = 19, type = "b")
plot(knn.error, xlab = "k value", ylab = "error rate", pch = 19, type = "b")
plot(knn.error, xlab = "k value", ylab = "error rate", pch = 23, type = "b")
plot(knn.error, xlab = "k value", ylab = "error rate", pch = 23, type = "b")
par(mfrow = (1,1))
par(mfrow=(1,1))
par(mfrow=c(1,1))
plot(knn.error, xlab = "k value", ylab = "error rate", pch = 23, type = "b")
plot(knn.error, xlab = "k value", ylab = "error rate", pch = 22, type = "b")
plot(knn.error, xlab = "k value", ylab = "error rate", pch = 12, type = "b")
plot(knn.error, xlab = "k value", ylab = "error rate", pch = 11, type = "b")
lr.fit <- glm(as.factor(crim_med) ~ ., data=train, family="binomial")
lr.probs <- predict(lr.fit, test, type="response")
lr.pred <- ifelse(lr.probs>.5, "1","0")
lr.test.error <- mean(lr.pred!=test$crim_med)
lr.test.error
lda.test.error
min.knn.error
set.seed(1)
x=rnorm(100)
y=x-2*x^2+rnorm(100)
plot(x,y)
library(boot)
set.seed(1)
Data <- data.frame(x, y)
fit.glm.1 <- glm(y ~ x)
cv.glm(Data, fit.glm.1)$delta[1]
set.seed(12345)
Data <- data.frame(x, y)
fit.glm.1 <- glm(y ~ x)
cv.glm(Data, fit.glm.1)$delta[1]
fit.glm.2 <- glm(y ~ poly(x, 2))
cv.glm(Data, fit.glm.2)$delta[1]
fit.glm.3 <- glm(y ~ poly(x, 3))
cv.glm(Data, fit.glm.3)$delta[1]
fit.glm.4 <- glm(y ~ poly(x, 4))
cv.glm(Data, fit.glm.4)$delta[1]
summary(fit.glm.2)
summary(fit.glm.1)
summary(fit.glm.2)
summary(fit.glm.3)
summary(fit.glm.4)
rm(list = list())
rm(list = ls())
#Question 1
#To predict  whether  a  given  suburb
# has a  crime  rate  above  or  below  the median.
# Explore logistic regression, LDA and kNN models using
# various subsets of the predictors.
##############################################################
library(MASS)
data(Boston)
set.seed(123)
summary(Boston)
# To numerize the class to 0 and 1
crim_med <- rep(0, length(Boston$crim))
crim_med[Boston$crim > median(Boston$crim)] <- 1
Boston <- data.frame(Boston, crim_med)
Boston$crim_med
# Create a set of box plots to check how each of the variable relates to crim_med
# ignore the chas variable and the crim variables
par(mfrow=c(2,6))
for(i in names(Boston)){
# excluding the own crime variables and the chas variable
if( grepl(i, pattern="^crim|^chas")){ next}
boxplot(eval(parse(text=i)) ~ crim_med, ylab=i, col=c("red", "blue"), varwidth=T)
}
#Train and Test sets
set.seed(12345)
variables = c("zn", "indus", "nox", "rm", "age", "dis", "rad", "tax", "ptratio", "black", "lstat", "medv", "crim_med")
rows = sample(x=nrow(Boston), size=.75*nrow(Boston))
train = Boston[rows, variables]
test = Boston[-rows, variables]
# Logistic Regression
lr.fit <- glm(as.factor(crim_med) ~ ., data=train, family="binomial")
lr.probs <- predict(lr.fit, test, type="response")
lr.pred <- ifelse(lr.probs>.5, "1","0")
lr.test.error <- mean(lr.pred!=test$crim_med)
lr.test.error
# LDA
lda.fit <- lda(train$crim_med ~ ., data = train)
lda.pred <- predict(lda.fit, test)
lda.test.error <- mean(lda.pred$class != test$crim_med)
lda.test.error
# KNN
set.seed(12345)
train.knn = train[, c("zn", "indus", "nox", "rm", "age", "dis", "rad", "tax", "ptratio", "black", "lstat", "medv")]
test.knn = test[, c("zn", "indus", "nox", "rm", "age", "dis", "rad", "tax", "ptratio", "black", "lstat", "medv")]
# kNN takes the training response variable seperately
train.knn.response = train$crim_med
test.knn.response = test$crim_med
library(class)
set.seed(123)
knn.pred = knn(train.knn, test.knn, train.knn.response, k = 1)
mean(knn.pred != test$crim_med)
knn.pred = NULL
knn.error = NULL
for(i in 1:dim(test.knn)[1]) {
set.seed(12345)
knn.pred = knn(train.knn, test.knn, train.knn.response, k = i)
knn.error[i] = mean(test$crim_med != knn.pred)
}
### find the minimum error rate
min.knn.error = min(knn.error)
print(min.knn.error)
### get the index of that error rate, which is the k
k = which(knn.error == min.knn.error)
print(k)
par(mfrow=c(1,1))
plot(knn.error, xlab = "k value", ylab = "error rate", pch = 11, type = "b")
summary(Boston)
#Question 1
#To predict  whether  a  given  suburb
# has a  crime  rate  above  or  below  the median.
# Explore logistic regression, LDA and kNN models using
# various subsets of the predictors.
##############################################################
library(MASS)
data(Boston)
set.seed(123)
summary(Boston)
# To numerize the class to 0 and 1
crim_med <- rep(0, length(Boston$crim))
crim_med[Boston$crim > median(Boston$crim)] <- 1
Boston <- data.frame(Boston, crim_med)
Boston$crim_med
# Create a set of box plots to check how each of the variable relates to crim_med
# ignore the chas variable and the crim variables
par(mfrow=c(2,6))
for(i in names(Boston)){
# excluding the own crime variables and the chas variable
if( grepl(i, pattern="^crim|^chas")){ next}
boxplot(eval(parse(text=i)) ~ crim_med, ylab=i, col=c("red", "blue"), varwidth=T)
}
#Train and Test sets
set.seed(12345)
variables = c("zn", "indus", "nox", "rm", "age", "dis", "rad", "tax", "ptratio", "black", "lstat", "medv", "crim_med")
rows = sample(x=nrow(Boston), size=.75*nrow(Boston))
train = Boston[rows, variables]
test = Boston[-rows, variables]
# Logistic Regression
lr.fit <- glm(as.factor(crim_med) ~ ., data=train, family="binomial")
lr.probs <- predict(lr.fit, test, type="response")
lr.pred <- ifelse(lr.probs>.5, "1","0")
lr.test.error <- mean(lr.pred!=test$crim_med)
lr.test.error
# LDA
lda.fit <- lda(train$crim_med ~ ., data = train)
lda.pred <- predict(lda.fit, test)
lda.test.error <- mean(lda.pred$class != test$crim_med)
lda.test.error
# KNN
set.seed(12345)
train.knn = train[, c("zn", "indus", "nox", "rm", "age", "dis", "rad", "tax", "ptratio", "black", "lstat", "medv")]
test.knn = test[, c("zn", "indus", "nox", "rm", "age", "dis", "rad", "tax", "ptratio", "black", "lstat", "medv")]
# kNN takes the training response variable seperately
train.knn.response = train$crim_med
test.knn.response = test$crim_med
library(class)
set.seed(123)
knn.pred = knn(train.knn, test.knn, train.knn.response, k = 1)
mean(knn.pred != test$crim_med)
knn.pred = NULL
knn.error = NULL
for(i in 1:dim(test.knn)[1]) {
set.seed(12345)
knn.pred = knn(train.knn, test.knn, train.knn.response, k = i)
knn.error[i] = mean(test$crim_med != knn.pred)
}
### find the minimum error rate
min.knn.error = min(knn.error)
print(min.knn.error)
### get the index of that error rate, which is the k
k = which(knn.error == min.knn.error)
print(k)
par(mfrow=c(1,1))
plot(knn.error, xlab = "k value", ylab = "error rate", pch = 11, type = "b")
